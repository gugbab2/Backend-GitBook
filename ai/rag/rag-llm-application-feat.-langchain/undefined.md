# 배경지식

## RAG(Retrieval Augmented Generation)

* 우리말로하면 검색-증강-생성

### Retrival&#x20;

* 데이터를 가져오는 것&#x20;
* 구체적으로는 "컴퓨터 시스템에 저장된 자료를 취득하는 것" 이라는 뜻도 있음
* 언어모델이 가지고 있지 않은 정보를 가져오는 것&#x20;
  * 언어모델이 아웃풋을 만드는데 필요한 정보를 제공하는 것&#x20;
  * 언어모델이 "답변 생성" 에는 능숙하지만, 답변 생성을 위한 모든 정보를 가지고 있지 않는다.&#x20;
    * 보안이 걸려 있는 사내자료&#x20;
    * LLM 모델은 학습 데이터가 최신 데이터를 포함하지 않는다.&#x20;

### Augmented&#x20;

* AR/VR 에 사용되는 것과 같은 단어&#x20;
* 마치 사실인 것 처럼 (할루시?)
* Retrival 된 데이터를 LLM 에게 주면서, "마치 이 정보를 아는 것처럼"

### Generation&#x20;

* 생성&#x20;
* 내가 가져온 데이터를 제공할테니, 이 정보를 아는 것 처럼

### 우리의 고민은?&#x20;

* 답변을 생성하는 것은 LLM 의 역할
* 우리는 데이터를 잘 가져와서,&#x20;
* LLM 에게 전달해야 함&#x20;

### 데이터를 잘 가져오려면?&#x20;

* 일단 잘 저장해야 함
* 잘 저장하는 것이 매우매우 어려움 (튜토리얼대로 따라한다고 해서 잘 저장되지 않음)&#x20;

### 잘 전달하려면?&#x20;

* 프롬프트를 잘 활용해야 함
* 문맥을 어떻게 제동할 것인가도 매우 중요
* 잘 가져오더라도 제대로 전달하지 못하면 LLM 이 올바른 답변을 주지 못함
  * LangChain 활용하면 장점&#x20;

## Vector Database 와 Embedding Model 성능 비교

### 사용자가 원하는 정보&#x20;

#### 1. 사용자의 질문과 관련있는 데이터&#x20;

* 관련이 있다는 것을 어떻게 판단할까?&#x20;
* 관련성 파악을 위해서 vector 를 활용한다.&#x20;
  * 단어 또는 문장 유사도를 파악해서 관련성을 측정함&#x20;

#### 2. Vector 를 생성하는 방법&#x20;

* Embedding 모델을 활용해서 vector 를 생성함&#x20;
* 문장에서 비슷한 단어가 자주 붙어있는 것을 학습&#x20;
  * 왕은 왕자의 아버지다.&#x20;
  * 여왕은 왕자의 어머니다.&#x20;
* "왕자의" 라는 단어 앞에 등장하는 "왕", "여왕" 은 유사항 가능성이 높다.&#x20;
* Embedding Projector

### Vector Database&#x20;

#### 1. Embedding 모델을 활용해 생성된 vector 를 저장&#x20;

* 단순히 vector 만 저장하면 안되고 metadata 도 같이 저장&#x20;
  * 이 항목이 상당히 중요한데, 뒤에 실습하면서 배울 예정&#x20;
  * 문서의 이름, 페이지 번호 등등을 같이 저장 -> LLM 이 생성하는 답변의 퀄리티가 상승함

#### 2. Vector 를 대상으로 유사도 검색 실시&#x20;

* **사용자의 질문과 가장 비슷한 문서를 가져오는 것 - Retrival**
  * 소득세법을 RAG 의 knowledge base 로 활용 예정&#x20;
  * 문서 전체를 활용하면 속도도 느리고, 토큰수 초과로 답변 생성이 안될수도 있음&#x20;
  * 문서를 chunking, 나눠서 저장해야 함
* **가져온 문서를 prompt 를 통해 LLM 에 제공 - Argumented**&#x20;
* **LLM 은 prompt 를 활용해서 답변 생성 - Generation**
